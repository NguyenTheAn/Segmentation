{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "Bkq39F0MK90w",
    "outputId": "a7191f9d-fcce-49a5-d191-a392facb31ed"
   },
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlolebNkLDkY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def jaccard_loss(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def castF(x):\n",
    "    return K.cast(x, K.floatx())\n",
    "\n",
    "def castB(x):\n",
    "    return K.cast(x, bool)\n",
    "\n",
    "def iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n",
    "    intersection = true * pred\n",
    "    notTrue = 1 - true\n",
    "    union = true + (notTrue * pred)\n",
    "\n",
    "    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n",
    "\n",
    "def IOU(true, pred): #any shape can go - can't be a loss function\n",
    "\n",
    "    tresholds = [0.5 + (i*.05)  for i in range(10)]\n",
    "\n",
    "    #flattened images (batch, pixels)\n",
    "    true = K.batch_flatten(true)\n",
    "    pred = K.batch_flatten(pred)\n",
    "    pred = castF(K.greater(pred, 0.5))\n",
    "\n",
    "    #total white pixels - (batch,)\n",
    "    trueSum = K.sum(true, axis=-1)\n",
    "    predSum = K.sum(pred, axis=-1)\n",
    "\n",
    "    #has mask or not per image - (batch,)\n",
    "    true1 = castF(K.greater(trueSum, 1))    \n",
    "    pred1 = castF(K.greater(predSum, 1))\n",
    "\n",
    "    #to get images that have mask in both true and pred\n",
    "    truePositiveMask = castB(true1 * pred1)\n",
    "\n",
    "    #separating only the possible true positives to check iou\n",
    "    testTrue = tf.boolean_mask(true, truePositiveMask)\n",
    "    testPred = tf.boolean_mask(pred, truePositiveMask)\n",
    "\n",
    "    #getting iou and threshold comparisons\n",
    "    iou = iou_loss_core(testTrue,testPred) \n",
    "    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n",
    "\n",
    "    #mean of thressholds for true positives and total sum\n",
    "    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n",
    "    truePositives = K.sum(truePositives)\n",
    "\n",
    "    #to get images that don't have mask in both true and pred\n",
    "    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n",
    "    trueNegatives = K.sum(trueNegatives) \n",
    "\n",
    "    return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n",
    "  \n",
    "def _unet(pretrained_weights=None, input_size=(64, 224, 3)):\n",
    "    inputs = Input(input_size, name=\"input_tensor\")\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(conv2)\n",
    "\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
    "    \n",
    "    \n",
    "# -------------------------------------------------------------\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(conv3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "# -------------------------------------------------------------\n",
    "    up4 = Conv2D(64, 2, activation='relu', padding='same',\n",
    "                 kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop3))\n",
    "    merge4 = concatenate([drop2, up4], axis=3)\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(merge4)\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(conv4)\n",
    "\n",
    "    up5 = Conv2D(32, 2, activation='relu', padding='same',\n",
    "                 kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv4))\n",
    "    merge5 = concatenate([conv1, up5], axis=3)\n",
    "    conv5 = Conv2D(32, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(merge5)\n",
    "    conv5 = Conv2D(32, 3, activation='relu', padding='same',\n",
    "                   kernel_initializer='he_normal')(conv5)\n",
    "    \n",
    "    conv6 = Conv2D(1, 1, activation='sigmoid', name=\"output_tensor\")(conv5)\n",
    "\n",
    "    model = Model(inputs, conv6)\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def unet(pretrained_weights=None, input_size=(256, 336, 3)):\n",
    "    model = _unet(pretrained_weights, input_size)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', metrics=[IOU])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_B2pUi2PLT4w",
    "outputId": "7c35bd66-cb34-4e94-f5c2-2df4b4b4b38b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 't\\\\data_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\optimizers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reading data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mx_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_FOLDER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Collected {} images\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\optimizers\\__init__.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mprepare_all_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\optimizers\\__init__.py\u001b[0m in \u001b[0;36mprepare_all_data\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0minputs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moutputs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_1_segmented\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 't\\\\data_1'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def _list_dir_abspath(dirpath):\n",
    "    assert os.path.isdir(dirpath), \"No such a directory\"\n",
    "    return [os.path.join(dirpath, elepath) for elepath in os.listdir(dirpath)]\n",
    "\n",
    "def prepare_all_data(folder):\n",
    "    list_subdataset = _list_dir_abspath(folder)\n",
    "    inputs = list()\n",
    "    outputs = list()\n",
    "    for subdataset in list_subdataset:\n",
    "        inputs_path = os.path.join(subdataset, \"data_1\")\n",
    "        outputs_path = os.path.join(subdataset, \"data_1_segmented\")\n",
    "        for filename in os.listdir(inputs_path):\n",
    "            inp = os.path.join(inputs_path, filename)\n",
    "            oup = os.path.join(outputs_path, filename)\n",
    "            if not (filename.endswith(\".png\") or filename.endswith(\".jpg\") or filename.endswith(\".jpeg\")):\n",
    "                continue\n",
    "            if os.path.isfile(inp) and os.path.isfile(oup):\n",
    "                inputs.append(inp)\n",
    "                outputs.append(oup)\n",
    "    return np.stack(inputs), np.stack(outputs)\n",
    "\n",
    "\n",
    "def find_images(path):\n",
    "    path = os.path.abspath(path)\n",
    "    assert os.path.isdir(path), (\"No such a directory\")\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    lst_imgs = []\n",
    "    for fil in os.listdir():\n",
    "        if not (fil.endswith(\".png\") or fil.endswith(\".jpg\") or fil.endswith(\".jpeg\")):\n",
    "            continue\n",
    "        lst_imgs.append(os.path.join(path, fil))\n",
    "    os.chdir(cwd)\n",
    "    return lst_imgs\n",
    "\n",
    "\n",
    "def create_dataset(folder):\n",
    "    return prepare_all_data(folder)\n",
    "\n",
    "\n",
    "class DataSequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size, datagen=None):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def preprecess_output(self, img_path, transform_parameters):\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        if self.datagen is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            img = self.datagen.apply_transform(img, transform_parameters)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        img = img/255.0\n",
    "        return np.round(img).astype(np.uint8)\n",
    "\n",
    "    def preprocess_input(self, img_path, transform_parameters):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = img[:,:,:3]\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        if self.datagen is not None:\n",
    "            img = self.datagen.apply_transform(img, transform_parameters)\n",
    "        img = img/255.0\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        if self.datagen is not None:\n",
    "            transform_parameters = self.datagen.get_random_transform((256,256,3))\n",
    "        else:\n",
    "            transform_parameters = None\n",
    "        inputs = {\n",
    "            \"input_tensor\": np.array([self.preprocess_input(file_name, transform_parameters) for file_name in batch_x])\n",
    "        }\n",
    "        outputs = {\n",
    "            \"output_tensor\": np.array([self.preprecess_output(file_name, transform_parameters) for file_name in batch_y])\n",
    "        }\n",
    "        return inputs, outputs\n",
    "\n",
    "\n",
    "TRAIN_FOLDER = \"test\"\n",
    "print(\"Reading data\")\n",
    "st = time.time()\n",
    "x_dataset, y_dataset = create_dataset(TRAIN_FOLDER)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=42)\n",
    "print(\"Collected {} images\".format(len(x_train)))\n",
    "# exit(0)\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_SIZE = x_train.shape[0]\n",
    "print(\"Reading data done in {}s\".format(time.time() - st))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# datagen = ImageDataGenerator(brightness_range=[0.7, 1.0], width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2)\n",
    "datagen = ImageDataGenerator(height_shift_range=0.1, width_shift_range=0.1, zoom_range=0.1)\n",
    "train_gen = DataSequence(x_train, y_train, BATCH_SIZE, datagen)\n",
    "test_gen = DataSequence(x_test, y_test, BATCH_SIZE)\n",
    "my_model = unet(input_size=(256, 256, 3))\n",
    "# my_model.compile(optimizer)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, period=1)\n",
    "logs = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(\"history.csv\", append=True)\n",
    "my_model.fit_generator(train_gen, steps_per_epoch=TRAIN_SIZE//BATCH_SIZE, validation_data=test_gen, \n",
    "                        callbacks=[csv_logger, logs, mc], epochs=100, max_queue_size=32, \n",
    "                        workers=12, initial_epoch=0)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
